{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 45867,
          "databundleVersionId": 6924515,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Combined",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'UBC-OCEAN:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F45867%2F6924515%2Fupload%2Fpublic.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240312%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240312T202455Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2bb2c47ed4456dc24c53bafd27749dd83e8aab6ff6bda18c692bfe04c5e4794d88ab3ed36ce9566ff079556ee388e4ff67b2991ebb40ae3874f83269a3795f3b0a1c09d3668aad3fddfdd82fd04f22c1a9452fd1fe89603652b9d33b68db7ca4fec3628623532bbbc615e8df60d1621958c656d9505f13fee725c4f06698d2bd659b39b95186df55c7e6b3b206d7d2b59ea2588bbc34ec4a47f040b5020f86c429f5bf411bdd16de59e8feb084468030324e360ba686c5209c2432db8a5356a4abfa713079b83263cd3747655e82d9c1c770fd93ae109ec19f53a995848500c7c55c1f5060985968eced2108e3112818b8951be0e78b289ed03994c5b74b50ee'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "CISrPmJzV2cG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-12T20:06:55.368179Z",
          "iopub.execute_input": "2024-03-12T20:06:55.368556Z",
          "iopub.status.idle": "2024-03-12T20:06:55.379328Z",
          "shell.execute_reply.started": "2024-03-12T20:06:55.368528Z",
          "shell.execute_reply": "2024-03-12T20:06:55.378121Z"
        },
        "trusted": true,
        "id": "C2ho1URoV2cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Image Rescaling"
      ],
      "metadata": {
        "id": "UMFh08rlV2cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to preprocess images\n",
        "def preprocess_images(input_folder, output_folder, target_size=(224, 224)):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Iterate through each image in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        # Construct the input path for the current image\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # Read the image using OpenCV\n",
        "        img = cv2.imread(input_path)\n",
        "\n",
        "        # Resize the image to the target size, using a target size of 224x224\n",
        "        resized_img = cv2.resize(img, target_size)\n",
        "\n",
        "\n",
        "        # Construct the output path for the resized image\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Save the resized image to the output folder\n",
        "        cv2.imwrite(output_path, resized_img)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T19:55:23.229144Z",
          "iopub.execute_input": "2024-03-12T19:55:23.229727Z",
          "iopub.status.idle": "2024-03-12T19:55:23.237427Z",
          "shell.execute_reply.started": "2024-03-12T19:55:23.229698Z",
          "shell.execute_reply": "2024-03-12T19:55:23.234738Z"
        },
        "trusted": true,
        "id": "1iUp9AmIV2cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = \"/kaggle/input/UBC-OCEAN/train_thumbnails\"\n",
        "output_folder = \"/kaggle/working/rescaled_images\"\n",
        "\n",
        "# Set the target size for resizing\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Preprocess images\n",
        "preprocess_images(input_folder, output_folder, target_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T19:55:23.238809Z",
          "iopub.execute_input": "2024-03-12T19:55:23.239195Z",
          "iopub.status.idle": "2024-03-12T19:57:11.746187Z",
          "shell.execute_reply.started": "2024-03-12T19:55:23.239168Z",
          "shell.execute_reply": "2024-03-12T19:57:11.745164Z"
        },
        "trusted": true,
        "id": "txaP7uLDV2cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Augmentation"
      ],
      "metadata": {
        "id": "OeXKY559V2cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_random_brightness(image_path):\n",
        "    \"\"\"\n",
        "    Adjust the brightness of an image to a random factor between 0.5 and 1.5 and save the result.\n",
        "    Parameters:\n",
        "        - image_path: The file path of the input image.\n",
        "    \"\"\"\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # Generate a random brightness factor between 0.5 and 1.5\n",
        "    random_factor = random.uniform(0.5, 1.5)\n",
        "    print(f\"Adjusting brightness by a factor of {random_factor}\")\n",
        "\n",
        "    # Create a brightness enhancer and apply the random factor\n",
        "    enhancer = ImageEnhance.Brightness(original_image)\n",
        "    BA_image = enhancer.enhance(random_factor)\n",
        "\n",
        "    # Create a new file name for the adjusted image\n",
        "    brightness_image_path = os.path.join(folder_path, f\"{file_name}_brightness_altered\")\n",
        "\n",
        "\n",
        "    # Save the adjusted image\n",
        "    BA_image.save(brightness_image_path)\n",
        "    print(f\"Brightness adjusted image saved: {brightness_image_path}\")\n",
        "\n",
        "\n",
        "def adjust_contrast(image_path):\n",
        "    \"\"\"\n",
        "    Adjust the contrast of an image to a random factor between 0.5 and 1.5 and save the result.\n",
        "    Parameters:\n",
        "        - image_path: The file path of the input image.\n",
        "    \"\"\"\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # Generate a random contrast factor between 0.5 and 1.5\n",
        "    random_factor = random.uniform(0.5, 1.5)\n",
        "    print(f\"Adjusting contrast by a factor of {random_factor}\")\n",
        "\n",
        "    # Create a contrast enhancer and apply the random factor\n",
        "    enhancer = ImageEnhance.Contrast(original_image)\n",
        "    contrast_image = enhancer.enhance(random_factor)\n",
        "\n",
        "    # Create a new file name for the adjusted image\n",
        "    contrast_image_path = os.path.join(folder_path, f\"{file_name}_contrast_altered\")\n",
        "\n",
        "    # Save the adjusted image\n",
        "    contrast_image.save(contrast_image_path)\n",
        "    print(f\"Contrast adjusted image saved: {contrast_image_path}\")\n",
        "\n",
        "\n",
        "def random_crop(image):\n",
        "    '''\n",
        "    Cropping the image in the center from a random margin from the borders\n",
        "    '''\n",
        "    margin = 1 / 3.5\n",
        "    width, height = image.size\n",
        "    start_x = int(random.uniform(0, width * margin))\n",
        "    start_y = int(random.uniform(0, height * margin))\n",
        "    end_x = int(random.uniform(width * (1 - margin), width))\n",
        "    end_y = int(random.uniform(height * (1 - margin), height))\n",
        "\n",
        "    cropped_image = image.crop((start_x, start_y, end_x, end_y))\n",
        "    return cropped_image\n",
        "\n",
        "\n",
        "def crop_images(image_path):\n",
        "    \"\"\"\n",
        "    Crop the given image and save the result.\n",
        "    Parameters:\n",
        "        - image_path: The file path of the input image.\n",
        "    \"\"\"\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # Apply random cropping\n",
        "    cropped_image = random_crop(original_image)\n",
        "\n",
        "    # Create a new file name for the cropped image\n",
        "    cropped_image_path = os.path.join(folder_path, f\"{file_name}_cropped\")\n",
        "\n",
        "    # Save the cropped image\n",
        "    cropped_image.save(cropped_image_path)\n",
        "    print(f\"Cropped image saved: {cropped_image_path}\")\n",
        "\n",
        "\n",
        "def horizontal_flip_images(image_path):\n",
        "    \"\"\"\n",
        "    Perform horizontal flipping on the given image and save the result.\n",
        "    Parameters:\n",
        "        - image_path: The file path of the input image.\n",
        "    \"\"\"\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # Perform horizontal flipping\n",
        "    flipped_image = original_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    # Create a new file name for the flipped image\n",
        "    flipped_image_path = os.path.join(folder_path, f\"{file_name}_hflipped\")\n",
        "\n",
        "    # Save the flipped image\n",
        "    flipped_image.save(flipped_image_path)\n",
        "    print(f\"Horizontal flipped image saved: {flipped_image_path}\")\n",
        "\n",
        "\n",
        "def vertical_flip_images(image_path):\n",
        "    \"\"\"\n",
        "    Perform vertical flipping on the given image and save the result.\n",
        "    Parameters:\n",
        "        - image_path: The file path of the input image.\n",
        "    \"\"\"\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # Perform vertical flipping\n",
        "    flipped_image = original_image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "\n",
        "    # Create a new file name for the flipped image\n",
        "    flipped_image_path = os.path.join(folder_path, f\"{file_name}_vflipped\")\n",
        "    # Save the flipped image\n",
        "    flipped_image.save(flipped_image_path)\n",
        "    print(f\"Vertical flipped image saved: {flipped_image_path}\")\n",
        "\n",
        "\n",
        "def add_random_gaussian_noise(folder_path):\n",
        "    \"\"\"\n",
        "    Add random Gaussian noise to an image and save the result.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: The file path of the input image.\n",
        "    - output_path: The file path for the output image.\n",
        "    - min_sigma: Minimum standard deviation of the Gaussian noise.\n",
        "    - max_sigma: Maximum standard deviation of the Gaussian noise.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Open the image using Pillow\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    #Adding noise\n",
        "    image_array = np.array(original_image)\n",
        "    # Ensure image is in float format\n",
        "    image_array = image_array.astype(float)\n",
        "\n",
        "    # Generate random Gaussian noise\n",
        "    sigma = random.uniform(50, 100)\n",
        "    noise = np.random.normal(0, sigma, image_array.shape)\n",
        "\n",
        "    # Add the noise to the image\n",
        "    noisy_image = image_array + noise\n",
        "\n",
        "    # Ensure values remain within the valid range [0, 255]\n",
        "    noisy_image = np.clip(noisy_image, 0, 255)\n",
        "\n",
        "    # Convert back to an image\n",
        "    noisy_image = noisy_image.astype(np.uint8)\n",
        "    noisy_image_pil = Image.fromarray(noisy_image)\n",
        "\n",
        "\n",
        "    # Create a new file name for the adjusted image\n",
        "    noisy_path = os.path.join(folder_path, f\"{file_name}_noisy\")\n",
        "\n",
        "    # Save the noisy image\n",
        "    noisy_image_pil.save(noisy_path)\n",
        "    print(f\"Added Gaussian noise with sigma = {sigma}. Saved to {noisy_path}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T20:09:32.15453Z",
          "iopub.execute_input": "2024-03-12T20:09:32.154903Z",
          "iopub.status.idle": "2024-03-12T20:09:32.173165Z",
          "shell.execute_reply.started": "2024-03-12T20:09:32.154875Z",
          "shell.execute_reply": "2024-03-12T20:09:32.17136Z"
        },
        "trusted": true,
        "id": "a5eMZjsiV2cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combined function\n",
        "\n",
        "def augment(folder_path):\n",
        "    \"\"\"\n",
        "    Combine all augmentation functions and apply them to images in the specified folder.\n",
        "    Parameters:\n",
        "        - folder_path: The path to the folder containing images to be augmented.\n",
        "    \"\"\"\n",
        "    file_list = os.listdir(folder_path)\n",
        "\n",
        "    # Iterate through each file in the folder\n",
        "    for file_name in file_list:\n",
        "        # Check if the file is an image\n",
        "        if file_name.lower().endswith('.png'):\n",
        "            # Create the full path to the image file\n",
        "            image_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            # Apply all augmentation functions\n",
        "            adjust_random_brightness(image_path)\n",
        "            adjust_contrast(image_path)\n",
        "            crop_images(image_path)\n",
        "            horizontal_flip_images(image_path)\n",
        "            vertical_flip_images(image_path)\n",
        "            add_random_gaussian_noise(image_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T20:20:48.282774Z",
          "iopub.execute_input": "2024-03-12T20:20:48.283105Z",
          "iopub.status.idle": "2024-03-12T20:20:48.289476Z",
          "shell.execute_reply.started": "2024-03-12T20:20:48.283081Z",
          "shell.execute_reply": "2024-03-12T20:20:48.288326Z"
        },
        "trusted": true,
        "id": "TbQEVBGgV2cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the function\n",
        "folder_path = '/kaggle/working/rescaled_images' #after scaling, images saved here\n",
        "augment(folder_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-12T20:20:52.765951Z",
          "iopub.execute_input": "2024-03-12T20:20:52.766256Z",
          "iopub.status.idle": "2024-03-12T20:20:53.057705Z",
          "shell.execute_reply.started": "2024-03-12T20:20:52.766233Z",
          "shell.execute_reply": "2024-03-12T20:20:53.056523Z"
        },
        "trusted": true,
        "id": "vSMj-NiHV2cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Creating validation images directory"
      ],
      "metadata": {
        "id": "eVlL4GmbWMw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the paths\n",
        "valid_images_path = '/kaggle/output/valid_images'\n",
        "\n",
        "# Create the validation folder if it doesn't exist\n",
        "os.makedirs(valid_images_path, exist_ok=True)\n",
        "\n",
        "# Get the list of all images in the train folder\n",
        "all_images = os.listdir(train_images_path)\n",
        "\n",
        "# Calculate the number of images to move (25%)\n",
        "num_images_to_move = int(0.25 * len(all_images))\n",
        "\n",
        "# Randomly select the images to move\n",
        "images_to_move = random.sample(all_images, num_images_to_move)\n",
        "\n",
        "# Move the selected images to the validation folder\n",
        "for image in images_to_move:\n",
        "    src_path = os.path.join(train_images_path, image)\n",
        "    dest_path = os.path.join(valid_images_path, image)\n",
        "    shutil.move(src_path, dest_path)"
      ],
      "metadata": {
        "id": "0DpcM1ELyE7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Dataloader"
      ],
      "metadata": {
        "id": "KBv7Ejr-WSi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After scaling and augmenting the data, we need to make sure the new images have their original unique numerical id + \"_flipped\"\n",
        "# The following class makes sure that pytorch can process the dataset during training, including accessing labels\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, csv_file):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.mapping_dict = self.create_mapping_dict()\n",
        "\n",
        "    def create_mapping_dict(self):\n",
        "        mapping_dict = {}\n",
        "        for idx in range(len(self.df)):\n",
        "            numeric_id = str(self.df.iloc[idx, 0])  # Assuming the numeric ID is in the first column\n",
        "            for suffix in ['blurred', 'noisy', 'hflipped', 'vflipped', 'cropped']:\n",
        "                augmented_id = f\"{numeric_id}_{suffix}\"\n",
        "                mapping_dict[augmented_id] = numeric_id\n",
        "        return mapping_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        # Count the total number of augmented images (5 times the number of original images)\n",
        "        return len(self.mapping_dict)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        augmented_id = list(self.mapping_dict.keys())[idx]\n",
        "        numeric_id = self.mapping_dict[augmented_id]\n",
        "\n",
        "        img_path = os.path.join(self.root_dir, f\"{numeric_id}_{augmented_id.split('_')[1]}.png\")\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        label = self.df.loc[self.df['numeric_id'] == int(numeric_id), 'label'].values[0]\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "Lmh67dqTc3mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset and dataloaders\n",
        "training_dataset = CustomDataset(csv_file='/kaggle/input/UBC-OCEAN/train.csv', root_dir='/kaggle/input/UBC-OCEAN/train_images')\n",
        "train_DL = DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#repeat for validation\n",
        "validation_dataset = CustomDataset(csv_file='/kaggle/input/UBC-OCEAN/train.csv', root_dir='/kaggle/input/UBC-OCEAN/valid_images')\n",
        "validation_DL = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "AGk3kVrYgHAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Training Loop"
      ],
      "metadata": {
        "id": "NHhUgRH_WXot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5paIaKpzt8uv"
      },
      "outputs": [],
      "source": [
        "# TODO: Build and train your network\n",
        "model = models.vgg16(pretrained = True) #Loading pre-trained network\n",
        "\n",
        "#Freeze parameters to avoid backpropagation\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "#Defining new untrained feed-forward network\n",
        "classifier = nn.Sequential(nn.Linear(25088,4096),\n",
        "                          nn.ReLU(), #activation function - ReLU is effective, computationally inexpensive\n",
        "                                     #and removes the vanishing gradient problem\n",
        "                          nn.Dropout(0.2),\n",
        "                          nn.Linear(4096, 256),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Dropout(0.2),\n",
        "                          nn.Linear(256, 64),\n",
        "                          nn.Dropout(0.2), #Removed 20% of data each time, good place to start\n",
        "                          nn.Linear(64, 6),  #Must be 6 because 6 = number of classes\n",
        "                          nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "classifier = classifier.to('cuda')\n",
        "model.classifier = classifier\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr = 0.01)\n",
        "\n",
        "#Training\n",
        "epochs = 5 #maybe change to 10 later on? depending on output\n",
        "train_loss = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #model.train()\n",
        "    for inputs, labels in train_DL:\n",
        "      model.train()\n",
        "      inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model.forward(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      validation_loss = 0\n",
        "      accuracy = 0\n",
        "      for inputs, labels in validation_DL:\n",
        "          inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "          outputs = model.forward(inputs)\n",
        "          running_valid_loss = criterion(outputs, labels).item()\n",
        "          validation_loss += running_valid_loss\n",
        "\n",
        "          ps = torch.exp(outputs)\n",
        "          top_p, top_class = ps.topk(1, dim = 1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}...\",\n",
        "          f\"Train loss: {train_loss/len(train_DL):.3f}...\"\n",
        "        f\"Validation loss: {validation_loss/len(validation_DL):.3f}...\"\n",
        "        f\"Validation accuracy: {accuracy:.3f}...\")\n",
        "    train_loss = 0\n",
        "\n"
      ]
    }
  ]
}